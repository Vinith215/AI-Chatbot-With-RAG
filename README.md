# AI Chatbot with Retrieval-Augmented Generation (RAG)
This project implements an AI-powered chatbot with Retrieval-Augmented Generation (RAG) capabilities. It combines a large language model (LLM) with a vector-based retrieval system to provide accurate and contextually relevant answers based on uploaded documents (e.g., PDFs). Built using Streamlit, LangChain, Groq, and HuggingFace embeddings, this chatbot is ideal for experimenting with RAG-based question-answering systems.

Features:
Interactive web interface powered by Streamlit.
Document processing and retrieval from PDFs using PyPDFLoader and a vector store.
Precise responses generated by Groq’s LLaMA3-8B model, enhanced with RAG.
Chat history maintained via Streamlit session state.
Efficient embeddings using HuggingFace’s all-MiniLM-L12-v2 model.

Prerequisites:
Before running the project, ensure you have the following:
Python 3.8 or higher
A Groq API key (sign up at Groq to obtain one)
The attention.pdf file (or any PDF you’d like the chatbot to reference) in the project directory.

Project Layout:

Setup Ulfor the Chatbot (Part 1):
1. Chatbot
2. Streamlit
3. Usage from the Ul
Connect to LLM (Part 2):
1. Langchain
2. Groq
Integrate RAG (Part 3):
1. Upload document(s)
2. Vector Embeddings
3. RAG (Retrieval Augmented Generation)
   
