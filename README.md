# AI Chatbot with Retrieval-Augmented Generation (RAG)

This project implements an AI-powered chatbot with Retrieval-Augmented Generation (RAG) capabilities. It combines a large language model (LLM) with a vector-based retrieval system to provide accurate and contextually relevant answers based on uploaded documents (e.g., PDFs). Built using Streamlit, LangChain, Groq, and HuggingFace embeddings, this chatbot is ideal for experimenting with RAG-based question-answering systems.

Features:

- Interactive web interface powered by Streamlit.
- Document processing and retrieval from PDFs using PyPDFLoader and a vector store.
- Precise responses generated by Groq’s LLaMA3-8B model, enhanced with RAG.
- Chat history maintained via Streamlit session state.
- Efficient embeddings using HuggingFace’s all-MiniLM-L12-v2 model.

Prerequisites:

- Before running the project, ensure you have the following:
- Python 3.8 or higher
- A Groq API key (sign up at Groq to obtain one)
- The attention.pdf file (or any PDF you’d like the chatbot to reference) in the project directory.

Installation:

1. Install Required Packages:
   - pipenv install streamlit langchain_groq langchain_community pypdf sentence-transformers
  
2. Set up a virtual environment
   - pipenv shell

3. Set up environmental variables: Create a .env file in the root directory and add your Groq API key:
   - GROQ_API_KEY=your-groq-api-key
  
4. Add a PDF File:
   - Place the attention.pdf file (or your own PDF) in the project directory. This will be used as the knowledge base for RAG.

6. Run the Applicatio:
   - streamlit run app.py

   
